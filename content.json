{"pages":[],"posts":[{"title":"네이버 부스트캠프 AI Tech 1주차 주간학습정리 AI Math - 벡터(Vector)","text":"2022년 1월 17일(월)부터 21일(금)까지 네이버 부스트캠프(boostcamp) AI Tech강의를 들으면서 중요하다고 생각되거나 짚고 넘어가야 할 핵심 내용들만 간단하게 메모한 내용입니다. 틀리거나 설명이 부족한 내용이 있을 수 있으며, 이는 학습을 진행하면서 꾸준히 내용을 수정하거나 추가해나갈 예정입니다. 벡터(Vector)벡터는 다음과 같이 다양하게 정의할 수 있다. 크기(길이)와 방향을 가지는 직선 벡터 공간(Vector Space)을 이루는 원소 공간에서의 한 점 원점으로부터의 상대적 위치 수를 원소로 가지는 리스트 또는 배열 종합하면 벡터는 수를 원소로 가지는 list 또는 array를 의미하며, $n$차원 공간에서의 한 점을 의미한다.이 때 점은 원점으로부터의 상대적 위치를 의미한다. numpy에서는 통상 행벡터로 값을 처리한다.Vector의 element는 $[x_1, x_2, …, x_d]$로 표현된다. 벡터는 표현 방식에 따라 두 가지로 분류할 수 있다. 행벡터(횡벡터): 벡터의 원소를 가로로 나열하여 표현 열벡터(종벡터): 벡터의 원소를 세로로 나열하여 표현 벡터의 연산(Vector Operation)벡터의 덧셈과 뺄셈같은 차원을 지닌 둘 이상의 벡터에서 서로 대응하는 성분끼리 덧셈 또는 뺄셈이 가능하다.즉, Shape이 같은 Vector간에는 동일한 위치의 element간의 연산이 일어난다.다른 벡터로부터의 상대적인 이동으로 나타낼 수도 있다.또한 더하는 벡터의 방향을 반대로 하면 뺄셈으로 연산할 수 있다. 벡터의 곱셈벡터의 모든 성분에 같은 수를 곱하는 것이다. 벡터에 수를 곱하면 벡터의 길이가 변한다.즉, 벡터(Vector)와 스칼라(Scalar) 연산을 하면 원점으로부터의 이동에 길이 변화가 생긴다. NLP에서 벡터를 어떻게 사용할까?언어를 처리하기 위해 단어를 벡터로 취급할 수도 있다.벡터 연산처럼 단어도 덧셈과 뺄셈 연산을 가능하도록 한다.예) ‘수도’ + ‘미국’ = ‘워싱턴 D.C.’ Norm원점으로부터의 거리를 의미하며, $|x|$로 표현된다.Norm의 정의에 따라 기하학적 성질 또는 표현이 달라진다. L1 Norm$||\\mathbf{x}||1 = \\sum{i=1}^{d}{|x_i|}$ 각 성분의 변화량의 절댓값을 모두 더한다. 맨해튼 거리와 같다. Robust 학습, Lasso 회귀 L2 Norm$ ||\\mathbf{x}||2 = \\sqrt{\\sum{i=1}^{d}{x_{i}^2}} $ 피타고라스 정리로 유클리드 거리를 계산한다. 시작점에서 끝점까지 직선으로 움직인 거리를 의미한다. Laplace 근사, Ridge 회귀 정규화항을 더하여 계수의 절댓값이나 제곱한 값이 커지지 않도록 만들어줄 때 유용하다. 두 Vector의 관계벡터 사이의 거리L1, L2 Norm을 이용하여 백터의 뺄셈을 사용해 두 벡터 사이의 거리를 계산할 수 있다.두 Vector x와 y의 거리는 $ZeroVector$와 $x−y$의 거리와 동일하다. $|x - y| = |y - x|$ 벡터 사이의 각도제2 코사인 법칙을 통해 각도도 구할 수 있는데, 각도는 오직 L2 Norm을 통해서만 구할 수 있다. 수식은 다음과 같다.$cosθ=\\frac{|x|_2^2+|y|_2^2−|x−y|_2^2}{2|x|2|y|2}$ 위 수식으로부터 분자를 쉽게 계산하는 방법은 내적이다. 이를 이용하면 다음과 같이 정리할 수 있다. $cosθ=\\frac{2\\langle x,y\\rangle}{2|x|_2|y|_2}$ $\\langle x,y\\rangle$는 각 Vector의 내적의 표현으로 $∑^d_{i=1}x^iy^i$와 동일하다.즉, 코사인 법칙의 분자를 쉽게 구할 수 있는 방법이 바로 벡터의 내적이다.내적은 np.inner함수로 구현할 수 있으며, 이를 코드로 옮기면 다음과 같다. 1234def angle(x, y) : v = np.inner(x, y) / (l2_norm(x) * l2_norm(y)) theta = np.arccos(v) return theta 벡터의 내적(Inner Product)같은 차원을 지닌 벡터에서 서로 대응하는 성분끼리 곱한 다음 이를 모두 더한 값이다. 그래서 벡터의 내적의 결과는 스칼라이다. 정사영(orthogonal projection)된 Vector의 길이와도 관련이 있는데, 벡터 $\\mathbf{x}$를 $\\mathbf{y}$에 정사영된 $Proj(\\mathbf{x})$는 코사인 법칙에 의해 $||\\mathbf{x}||\\cos\\theta$와 동일하게 된다.$Proj(\\mathbf{x})$를 $||\\mathbf{y}||$만큼 조정한 값을 의미한다. 이를 통해 유사도를 측정하는데 사용할 수도 있다. 코사인 유사도벡터의 내적을 사용한 코사인 유사도를 통해 두 벡터의 유사도를 구할 수 있다. $\\cos{(a, b)} = \\frac{\\langle a, b\\rangle} {|a| |b|}$ 코사인 유사도는 항상 -1 이상 1 이하의 값을 지닌다. 두 벡터의 코사인 유사도가 높다는 건 두 벡터의 서로 유사하다는 의미이다. NLP에서의 코사인 유사도텍스트 분석 시 벡터 기반의 단어 또는 문장 간의 관계성을 파악하는 데 활용할 수 있다.","link":"/2022/02/06/20220206-AI-Math-Vector/"}],"tags":[],"categories":[]}